# Transformer

## Transformer

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2021-11-05**|**A `Rosetta stone' for the population dynamics of spiking neuron networks**|Gianni V. Vinci et.al.|[2111.03621v1](http://arxiv.org/abs/2111.03621v1)|null|
|**2021-11-05**|**Second Degree Model for Multi-Compression and Recovery of Distributed Signals**|Pablo Soto-Quiros et.al.|[2111.03614v1](http://arxiv.org/abs/2111.03614v1)|null|
|**2021-11-05**|**Sexism Identification in Tweets and Gabs using Deep Neural Networks**|Amikul Kalra et.al.|[2111.03612v1](http://arxiv.org/abs/2111.03612v1)|null|
|**2021-11-05**|**Morphogenesis of street networks. A reaction-diffusion system for self-organized cities**|Michele Tirico et.al.|[2111.03544v1](http://arxiv.org/abs/2111.03544v1)|null|
|**2021-11-05**|**Guaranteed blind deconvolution and demixing via hierarchically sparse reconstruction**|Axel Flinth et.al.|[2111.03486v1](http://arxiv.org/abs/2111.03486v1)|null|
|**2021-11-05**|**Improving Visual Quality of Image Synthesis by A Token-based Generator with Transformers**|Yanhong Zeng et.al.|[2111.03481v1](http://arxiv.org/abs/2111.03481v1)|null|
|**2021-11-05**|**Sampling Equivariant Self-attention Networks for Object Detection in Aerial Images**|Guo-Ye Yang et.al.|[2111.03420v1](http://arxiv.org/abs/2111.03420v1)|null|
|**2021-11-05**|**A Deep Learning Generative Model Approach for Image Synthesis of Plant Leaves**|Alessandrop Benfenati et.al.|[2111.03388v1](http://arxiv.org/abs/2111.03388v1)|null|
|**2021-11-05**|**Hepatic vessel segmentation based on 3Dswin-transformer with inductive biased multi-head self-attention**|Mian Wu et.al.|[2111.03368v1](http://arxiv.org/abs/2111.03368v1)|null|
|**2021-11-05**|**Semi-measures and their Fourier transform**|Timo Spindeler et.al.|[2111.03304v1](http://arxiv.org/abs/2111.03304v1)|null|
|**2021-11-05**|**Multi-breather solutions to the Sasa-Satsuma equation**|Chengfa Wu et.al.|[2111.03266v1](http://arxiv.org/abs/2111.03266v1)|null|
|**2021-11-05**|**Context-Aware Transformer Transducer for Speech Recognition**|Feng-Ju Chang et.al.|[2111.03250v1](http://arxiv.org/abs/2111.03250v1)|null|
|**2021-11-05**|**Analysis of Sensing Spectral for Signal Recovery Under a Generalized Linear Model**|Junjie Ma et.al.|[2111.03237v1](http://arxiv.org/abs/2111.03237v1)|null|
|**2021-11-05**|**The Fourier Transform of Restrictions of Functions on the Slice**|Shravas Rao et.al.|[2111.03213v1](http://arxiv.org/abs/2111.03213v1)|null|
|**2021-11-04**|**An Empirical Study of the Effectiveness of an Ensemble of Stand-alone Sentiment Detection Tools for Software Engineering Datasets**|Gias Uddin et.al.|[2111.03196v1](http://arxiv.org/abs/2111.03196v1)|null|
|**2021-11-04**|**Notes on massless scalar field partition functions, modular invariance and Eisenstein series**|Francesco Alessio et.al.|[2111.03164v1](http://arxiv.org/abs/2111.03164v1)|null|
|**2021-11-04**|**A Validated Method for Predicting Small Molecule Ionization Sites using Gibb's Free Energies**|Jessica L. Bade et.al.|[2111.03141v1](http://arxiv.org/abs/2111.03141v1)|null|
|**2021-11-04**|**How Do Neural Sequence Models Generalize? Local and Global Context Cues for Out-of-Distribution Prediction**|Anthony Bau et.al.|[2111.03108v1](http://arxiv.org/abs/2111.03108v1)|null|
|**2021-11-04**|**Space-time wave packets localized in all dimensions**|Murat Yessenov et.al.|[2111.03095v1](http://arxiv.org/abs/2111.03095v1)|null|
|**2021-11-04**|**Correlation of microstructural and multicaloric properties of suction-cast Ni-Mn-In**|Lukas Pfeuffer et.al.|[2111.03092v1](http://arxiv.org/abs/2111.03092v1)|null|

## Vision Transformer

| Publish Date | Title | Authors | PDF | Code |
|:---------|:-----------------------|:---------|:------|:------|
|**2021-11-04**|**Generalized Radiograph Representation Learning via Cross-supervision between Images and Free-text Radiology Reports**|Hong-Yu Zhou et.al.|[2111.03452v1](http://arxiv.org/abs/2111.03452v1)|null|
|**2021-11-03**|**TranSMS: Transformers for Super-Resolution Calibration in Magnetic Particle Imaging**|Alper Güngör et.al.|[2111.02163v1](http://arxiv.org/abs/2111.02163v1)|null|
|**2021-11-02**|**PatchGame: Learning to Signal Mid-level Patches in Referential Games**|Kamal Gupta et.al.|[2111.01785v1](http://arxiv.org/abs/2111.01785v1)|**[link](https://github.com/kampta/patchgame)**|
|**2021-11-02**|**Low-Rank+Sparse Tensor Compression for Neural Networks**|Cole Hawkins et.al.|[2111.01697v1](http://arxiv.org/abs/2111.01697v1)|null|
|**2021-11-02**|**Can Vision Transformers Perform Convolution?**|Shanda Li et.al.|[2111.01353v2](http://arxiv.org/abs/2111.01353v2)|null|
|**2021-11-02**|**Federated Split Vision Transformer for COVID-19 CXR Diagnosis using Task-Agnostic Training**|Sangjoon Park et.al.|[2111.01338v2](http://arxiv.org/abs/2111.01338v2)|null|
|**2021-11-01**|**HRViT: Multi-Scale High-Resolution Vision Transformer**|Jiaqi Gu et.al.|[2111.01236v1](http://arxiv.org/abs/2111.01236v1)|null|
|**2021-10-28**|**Scatterbrain: Unifying Sparse and Low-rank Attention Approximation**|Beidi Chen et.al.|[2110.15343v1](http://arxiv.org/abs/2110.15343v1)|**[link](https://github.com/hazyresearch/scatterbrain)**|
|**2021-10-28**|**Blending Anti-Aliasing into Vision Transformer**|Shengju Qian et.al.|[2110.15156v1](http://arxiv.org/abs/2110.15156v1)|null|
|**2021-10-28**|**Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training**|Zhengda Bian et.al.|[2110.14883v1](http://arxiv.org/abs/2110.14883v1)|**[link](https://github.com/hpcaitech/colossalai)**|
|**2021-10-27**|**Detecting Dementia from Speech and Transcripts using Transformers**|Loukas Ilias et.al.|[2110.14769v1](http://arxiv.org/abs/2110.14769v1)|null|
|**2021-10-27**|**Vision Transformer for Classification of Breast Ultrasound Images**|Behnaz Gheflati et.al.|[2110.14731v1](http://arxiv.org/abs/2110.14731v1)|null|
|**2021-10-25**|**History Aware Multimodal Transformer for Vision-and-Language Navigation**|Shizhe Chen et.al.|[2110.13309v1](http://arxiv.org/abs/2110.13309v1)|null|
|**2021-10-25**|**MVT: Multi-view Vision Transformer for 3D Object Recognition**|Shuo Chen et.al.|[2110.13083v1](http://arxiv.org/abs/2110.13083v1)|null|
|**2021-10-24**|**CvT-ASSD: Convolutional vision-Transformer Based Attentive Single Shot MultiBox Detector**|Weiqiang Jin et.al.|[2110.12364v1](http://arxiv.org/abs/2110.12364v1)|**[link](https://github.com/albert-jin/cvt-assd)**|
|**2021-10-22**|**How and When Adversarial Robustness Transfers in Knowledge Distillation?**|Rulin Shao et.al.|[2110.12072v1](http://arxiv.org/abs/2110.12072v1)|null|
|**2021-10-22**|**SOFT: Softmax-free Transformer with Linear Complexity**|Jiachen Lu et.al.|[2110.11945v2](http://arxiv.org/abs/2110.11945v2)|null|
|**2021-10-21**|**Vis-TOP: Visual Transformer Overlay Processor**|Wei Hu et.al.|[2110.10957v1](http://arxiv.org/abs/2110.10957v1)|null|
|**2021-10-19**|**Bilateral-ViT for Robust Fovea Localization**|Sifan Song et.al.|[2110.09860v1](http://arxiv.org/abs/2110.09860v1)|null|
|**2021-10-19**|**SSAST: Self-Supervised Audio Spectrogram Transformer**|Yuan Gong et.al.|[2110.09784v1](http://arxiv.org/abs/2110.09784v1)|null|

